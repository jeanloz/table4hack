{
  "nbformat": 4,
  "cells": [
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\nAutograd\n========\n\nAutograd is now a core torch package for automatic differentiation.\nIt uses a tape based system for automatic differentiation.\n\nIn the forward phase, the autograd tape will remember all the operations\nit executed, and in the backward phase, it will replay the operations.\n\nTensors that track history\n--------------------------\n\nIn autograd, if any input ``Tensor`` of an operation has ``requires_grad=True``,\nthe computation will be tracked. After computing the backward pass, a gradient\nw.r.t. this tensor is accumulated into ``.grad`` attribute.\n\nThere\u2019s one more class which is very important for autograd\nimplementation - a ``Function``. ``Tensor`` and ``Function`` are\ninterconnected and build up an acyclic graph, that encodes a complete\nhistory of computation. Each variable has a ``.grad_fn`` attribute that\nreferences a function that has created a function (except for Tensors\ncreated by the user - these have ``None`` as ``.grad_fn``).\n\nIf you want to compute the derivatives, you can call ``.backward()`` on\na ``Tensor``. If ``Tensor`` is a scalar (i.e. it holds a one element\ntensor), you don\u2019t need to specify any arguments to ``backward()``,\nhowever if it has more elements, you need to specify a ``grad_output``\nargument that is a tensor of matching shape.\n\n"
      ],
      "metadata": {}
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a tensor and set requires_grad=True to track computation with it\n\n"
      ],
      "metadata": {}
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\nprint(x)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "print(x.data)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "print(x.grad_fn)  # we've created x ourselves"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do an operation of x:\n\n"
      ],
      "metadata": {}
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "y = x + 2\nprint(y)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "y was created as a result of an operation,\nso it has a grad_fn\n\n"
      ],
      "metadata": {}
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "print(y.grad_fn)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More operations on y:\n\n"
      ],
      "metadata": {}
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "z = y * y * 3\nout = z.mean()\n\nprint(z, out)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\nflag in-place. The input flag defaults to ``True`` if not given.\n\n"
      ],
      "metadata": {}
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "a = torch.randn(2, 2)\na = ((a * 3) / (a - 1))\nprint(a.requires_grad)\na.requires_grad_(True)\nprint(a.requires_grad)\nb = (a * a).sum()\nprint(b.grad_fn)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradients\n---------\n\nlet's backprop now and print gradients d(out)/dx\n\n"
      ],
      "metadata": {}
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "out.backward()\nprint(x.grad)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, gradient computation flushes all the internal buffers\ncontained in the graph, so if you even want to do the backward on some\npart of the graph twice, you need to pass in ``retain_variables = True``\nduring the first pass.\n\n"
      ],
      "metadata": {}
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\ny = x + 2\ny.backward(torch.ones(2, 2), retain_graph=True)\n# the retain_variables flag will prevent the internal buffers from being freed\nprint(x.grad)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "z = y * y\nprint(z)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "just backprop random gradients\n\n"
      ],
      "metadata": {}
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "gradient = torch.randn(2, 2)\n\n# this would fail if we didn't specify\n# that we want to retain variables\ny.backward(gradient)\n\nprint(x.grad)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also stops autograd from tracking history on Tensors\nwith requires_grad=True by wrapping the code block in\n``with torch.no_grad():``\n\n"
      ],
      "metadata": {}
    },
    {
      "execution_count": null,
      "cell_type": "code",
      "source": [
        "print(x.requires_grad)\nprint((x ** 2).requires_grad)\n\nwith torch.no_grad():\n\tprint((x ** 2).requires_grad)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": false
      }
    }
  ],
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "version": "3.5.5",
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "mimetype": "text/x-python"
    },
    "kernelspec": {
      "language": "python",
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}